# NaviGPT Project Structure

## ğŸ“ Directory Organization

```
NaviGPT/
â”œâ”€â”€ ğŸ“„ Documentation
â”‚   â”œâ”€â”€ README.md                      # Project overview
â”‚   â”œâ”€â”€ DEVELOPMENT_PHASES.md          # Complete development phases
â”‚   â”œâ”€â”€ PROJECT_STRUCTURE.md           # This file
â”‚   â”œâ”€â”€ PHASE1_TESTING_SETUP.md        # Phase 1 setup guide
â”‚   â”œâ”€â”€ PHASE2_README.md               # Phase 2 technical details
â”‚   â”œâ”€â”€ PHASE3_COREML_MODELS.md        # Model integration guide
â”‚   â”œâ”€â”€ CONFIGURATION_SETUP.md         # API configuration
â”‚   â””â”€â”€ LICENSE                        # CC BY-NC 4.0 license
â”‚
â”œâ”€â”€ ğŸ› ï¸ Configuration
â”‚   â”œâ”€â”€ .env.example                   # Template for API keys
â”‚   â”œâ”€â”€ .gitignore                     # Git ignore rules
â”‚   â””â”€â”€ setup-config.sh                # Configuration setup script
â”‚
â”œâ”€â”€ ğŸ”§ Build Scripts
â”‚   â”œâ”€â”€ add_to_xcode.py               # Add files to Xcode
â”‚   â”œâ”€â”€ add_phase_files_to_xcode.py   # Phase files integration
â”‚   â”œâ”€â”€ add_file_xcode.scpt           # AppleScript helper
â”‚   â”œâ”€â”€ add_files_to_xcode.scpt       # Batch file addition
â”‚   â”œâ”€â”€ run_tests.sh                  # Test runner
â”‚   â”œâ”€â”€ fix_path.py                   # Path fixer utility
â”‚   â””â”€â”€ remove_refs.py                # Reference cleanup
â”‚
â””â”€â”€ ğŸ“± NaviGPT_build_from_here/
    â”œâ”€â”€ NaviGPT.xcodeproj             # Xcode project
    â”‚
    â”œâ”€â”€ ğŸ¯ NaviGPT/ (Main Target)
    â”‚   â”‚
    â”‚   â”œâ”€â”€ ğŸ—ï¸ Core App (Phase 0)
    â”‚   â”‚   â”œâ”€â”€ Intern1App.swift      # App entry point
    â”‚   â”‚   â”œâ”€â”€ ContentView.swift     # Main UI
    â”‚   â”‚   â””â”€â”€ NaviGPTCore.swift     # Core logic (Phase 1)
    â”‚   â”‚
    â”‚   â”œâ”€â”€ ğŸ“¸ Camera & LiDAR (Phase 0)
    â”‚   â”‚   â”œâ”€â”€ LiDARCameraView.swift           # LiDAR UI
    â”‚   â”‚   â”œâ”€â”€ LiDARCameraViewController.swift # LiDAR controller
    â”‚   â”‚   â”œâ”€â”€ CameraPreviewView.swift         # Camera preview
    â”‚   â”‚   â””â”€â”€ cameraManager.swift             # Camera operations
    â”‚   â”‚
    â”‚   â”œâ”€â”€ ğŸ—ºï¸ Maps & Navigation (Phase 0)
    â”‚   â”‚   â”œâ”€â”€ MapsView.swift          # Map UI
    â”‚   â”‚   â””â”€â”€ mapsManager.swift       # Map functionality
    â”‚   â”‚
    â”‚   â”œâ”€â”€ ğŸ¤ Speech & Audio (Phase 0)
    â”‚   â”‚   â”œâ”€â”€ SpeechManager.swift     # Text-to-speech
    â”‚   â”‚   â””â”€â”€ speechRecognizer.swift  # Speech-to-text
    â”‚   â”‚
    â”‚   â”œâ”€â”€ ğŸ¤– LLM Integration (Phase 0)
    â”‚   â”‚   â””â”€â”€ llmManager.swift        # OpenAI GPT-4 integration
    â”‚   â”‚
    â”‚   â”œâ”€â”€ âš™ï¸ Configuration (Phase 1)
    â”‚   â”‚   â”œâ”€â”€ ConfigManager.swift     # Multi-source config
    â”‚   â”‚   â””â”€â”€ Config.plist.example    # Config template
    â”‚   â”‚
    â”‚   â”œâ”€â”€ ğŸ“Š Models/ (Phase 1 & 2)
    â”‚   â”‚   â”œâ”€â”€ ObstacleInfo.swift      # Obstacle detection (Phase 1)
    â”‚   â”‚   â”œâ”€â”€ NavigationContext.swift # Navigation state (Phase 1)
    â”‚   â”‚   â”œâ”€â”€ VisionModels.swift      # Vision models (Phase 1)
    â”‚   â”‚   â”œâ”€â”€ ModelTypes.swift        # ML type system (Phase 2)
    â”‚   â”‚   â”‚
    â”‚   â”‚   â””â”€â”€ CoreML/                 # CoreML models (Phase 3)
    â”‚   â”‚       â”œâ”€â”€ README.md           # Models guide
    â”‚   â”‚       â”œâ”€â”€ YOLOv8.mlmodel      # Object detection (to be added)
    â”‚   â”‚       â”œâ”€â”€ DepthEstimation.mlmodel  # Depth ML (to be added)
    â”‚   â”‚       â””â”€â”€ SceneClassifier.mlmodel  # Scene classification (to be added)
    â”‚   â”‚
    â”‚   â”œâ”€â”€ ğŸ”¬ Services/ (Phase 2)
    â”‚   â”‚   â”œâ”€â”€ CoreMLModelManager.swift        # Model lifecycle
    â”‚   â”‚   â”œâ”€â”€ VisionModelProcessor.swift      # Object detection
    â”‚   â”‚   â””â”€â”€ DepthEstimationProcessor.swift  # Depth processing
    â”‚   â”‚
    â”‚   â”œâ”€â”€ ğŸ§ª Tests/
    â”‚   â”‚   â”œâ”€â”€ ConfigManagerTests.swift  # Config tests (Phase 1)
    â”‚   â”‚   â””â”€â”€ ObstacleInfoTests.swift   # Obstacle tests (Phase 1)
    â”‚   â”‚
    â”‚   â”œâ”€â”€ ğŸ¨ Assets
    â”‚   â”‚   â”œâ”€â”€ Assets.xcassets         # App assets
    â”‚   â”‚   â””â”€â”€ Preview Content/        # Preview assets
    â”‚   â”‚
    â”‚   â””â”€â”€ ğŸ“‹ Resources
    â”‚       â””â”€â”€ Base.lproj/
    â”‚           â””â”€â”€ Info.plist          # App info
    â”‚
    â”œâ”€â”€ ğŸ§ª Intern1Tests/ (Test Target)
    â”‚   â”œâ”€â”€ Intern1Tests.swift         # Base tests (Phase 0)
    â”‚   â””â”€â”€ Phase2Tests.swift          # ML tests (Phase 2)
    â”‚
    â”œâ”€â”€ ğŸ§ª Intern1UITests/ (UI Test Target)
    â”‚   â”œâ”€â”€ Intern1UITests.swift
    â”‚   â””â”€â”€ Intern1UITestsLaunchTests.swift
    â”‚
    â””â”€â”€ ğŸ“¦ NaviGPTTests/ (Additional Tests)
        â””â”€â”€ (Test files)
```

## ğŸ“Š File Count by Phase

### Phase 0: Initial Setup (10 files)
- Core app structure: 3 files
- Camera & LiDAR: 4 files
- Maps: 2 files
- Speech: 2 files
- LLM: 1 file

### Phase 1: Configuration & Testing (6 files)
- Configuration: 2 files
- Models: 3 files
- Tests: 2 files
- Documentation: 2 files

### Phase 2: CoreML Integration (7 files)
- Services: 3 files
- Models: 1 file
- Tests: 1 file
- Documentation: 2 files

### Phase 3: Planned (3+ model files)
- CoreML models: TBD
- Camera pipeline: TBD
- LiDAR enhancement: TBD

## ğŸ¯ Key Entry Points

### Main Application
- **Entry Point**: `Intern1App.swift`
- **Main View**: `ContentView.swift`
- **Core Logic**: `NaviGPTCore.swift`

### Configuration
- **Config Manager**: `ConfigManager.swift`
- **Environment**: `.env` (create from `.env.example`)

### ML/AI Components
- **Model Manager**: `Services/CoreMLModelManager.swift`
- **Vision Processing**: `Services/VisionModelProcessor.swift`
- **Depth Processing**: `Services/DepthEstimationProcessor.swift`

### Testing
- **Base Tests**: `Intern1Tests/Intern1Tests.swift`
- **ML Tests**: `Intern1Tests/Phase2Tests.swift`
- **Config Tests**: `NaviGPT/Tests/ConfigManagerTests.swift`

## ğŸ” Finding Specific Functionality

### LiDAR & Camera
```
NaviGPT/LiDARCameraView.swift          - UI for LiDAR camera
NaviGPT/LiDARCameraViewController.swift - LiDAR controller
NaviGPT/cameraManager.swift            - Camera operations
```

### Navigation & Maps
```
NaviGPT/MapsView.swift     - Map UI
NaviGPT/mapsManager.swift  - Map functionality
```

### Speech & Audio
```
NaviGPT/SpeechManager.swift    - Text-to-speech
NaviGPT/speechRecognizer.swift - Speech recognition
```

### AI & ML
```
NaviGPT/llmManager.swift                          - LLM (GPT-4)
Services/CoreMLModelManager.swift                 - CoreML models
Services/VisionModelProcessor.swift               - Object detection
Services/DepthEstimationProcessor.swift           - Depth estimation
```

### Data Models
```
Models/ObstacleInfo.swift      - Obstacle data
Models/NavigationContext.swift - Navigation state
Models/VisionModels.swift      - Vision data
Models/ModelTypes.swift        - ML type system
```

## ğŸ“ Documentation Map

### Getting Started
1. **README.md** - Project overview and quick start
2. **CONFIGURATION_SETUP.md** - Set up API keys
3. **DEVELOPMENT_PHASES.md** - Understand project phases

### Phase-Specific Guides
1. **PHASE1_TESTING_SETUP.md** - Phase 1 setup
2. **PHASE2_README.md** - Phase 2 technical details
3. **PHASE3_COREML_MODELS.md** - Model integration

### Project Structure
- **PROJECT_STRUCTURE.md** - This file

## ğŸ”— Quick Navigation

### To build the project:
```bash
cd NaviGPT_build_from_here
open NaviGPT.xcodeproj
# Press Cmd+B to build
```

### To run tests:
```bash
cd NaviGPT_build_from_here
xcodebuild test -scheme Intern1
# Or press Cmd+U in Xcode
```

### To add CoreML models:
```bash
# 1. Place .mlmodel files in NaviGPT/Models/CoreML/
# 2. In Xcode: Right-click NaviGPT â†’ Add Files â†’ Select models
# 3. Ensure "NaviGPT" target is checked
```

## ğŸ—ï¸ Build Configuration

### Targets
- **NaviGPT** - Main app target
- **NaviGPTTests** - Unit tests
- **NaviGPTUITests** - UI tests

### Schemes
- **Intern1** - Main build scheme

### Requirements
- Xcode 15.4+
- iOS 17.2+
- iPhone 12 Pro or later (for LiDAR)
- Swift 5.0

## ğŸ“¦ Dependencies

### System Frameworks
- **CoreML** - ML model execution
- **Vision** - Computer vision
- **ARKit** - LiDAR & AR features
- **AVFoundation** - Camera & audio
- **MapKit** - Maps & navigation
- **SwiftUI** - UI framework

### External Dependencies
- **OpenAI API** - GPT-4 integration (requires API key)

## ğŸ¨ Asset Organization

### Assets.xcassets
- App icons
- Colors
- Images

### Preview Content
- Preview assets for SwiftUI previews
- Development-only assets

---

**Last Updated**: November 2024
**For Questions**: See README.md or DEVELOPMENT_PHASES.md
